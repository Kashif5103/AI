{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgh6kAl1ky19Yy2BoYcPHv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kashif5103/AI/blob/main/Huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging face"
      ],
      "metadata": {
        "id": "9NW7NJ4hbKI4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhGmHXJMa94t"
      },
      "outputs": [],
      "source": [
        "# installing the required model\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification\n",
        "\n",
        "we can classify the text using the Hugging face pre build pipeline we will extensily use pipeline"
      ],
      "metadata": {
        "id": "83DSbXn8bYqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentimental Analysisi"
      ],
      "metadata": {
        "id": "4E2FErrPbmxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import pipeline\n",
        "pipe=pipeline(\"text-classification\")\n",
        "pipe(\"This movie is very boring\")"
      ],
      "metadata": {
        "id": "k5lUHBhWbTDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also pass the string in the form of list"
      ],
      "metadata": {
        "id": "grwxHTsPb0Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline for the text classification\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "pipe=pipeline(\"sentiment-analysis\")\n",
        "pipe([\"This University is awesome\",\"This university is awful\"])"
      ],
      "metadata": {
        "id": "wmL6b2d1b1pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe(\"I don't know where i am going\")"
      ],
      "metadata": {
        "id": "IKHGArxvb9Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "8QuebQxRb-eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text to the summarized\n",
        "input_text=\"Start providing your input.it could be a sentance and or \\n a paraegeapht  Tokenization is the process of breaking down a text into smaller \\n units called tokens. Tokens can be words, subwords, or characters, depending on the \\n granularity required. This process is essential for converting raw text into a format that \\n  can be understood and processed by machine learning algorithms.\"\n",
        "print(input_text)"
      ],
      "metadata": {
        "id": "Y19C7Ul4cHn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use bart in pytorch\n",
        "summarizer=transformers.pipeline(\"summarization\")\n",
        "summarizer(input_text,max_length=30,min_length=5)"
      ],
      "metadata": {
        "id": "ykvH_xbHcPcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Name Entity Recogination\n",
        "\n",
        "**What is Named Entity Recognition (NER)**? NER is a subtask of information extraction that seeks to locate and classify entities in text into predefined categories. For example, in the sentence \"Apple Inc. was founded by Steve Jobs in Cupertino,\" NER will identify:\n",
        "\n",
        "\"Apple Inc.\" as an Organization, \"Steve Jobs\" as a Person, \"Cupertino\" as a Location."
      ],
      "metadata": {
        "id": "NT2bqUIOcUN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the NER pipeline\n",
        "nlp = pipeline(\"ner\")\n",
        "\n",
        "# Example text\n",
        "example = \"Apple Inc. was founded by Steve Jobs in Cupertino.\"\n",
        "\n",
        "# Perform NER\n",
        "ner_results = nlp(example)\n",
        "\n",
        "# Print the results\n",
        "print(ner_results)\n"
      ],
      "metadata": {
        "id": "52-kvngGccaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Classification\n",
        "\n",
        "Image classification in NLP involves categorizing images using textual information. This could include:\n",
        "\n",
        "Captions or Descriptions: Textual descriptions of the images.\n",
        "Metadata: Tags, keywords, or other textual data associated with images."
      ],
      "metadata": {
        "id": "DrtHkXfQclfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the image\n",
        "from PIL import Image\n",
        "# image path\n",
        "image_path=\"/content/pexels-chevanon-1108099.png\"\n",
        "# open using the PIL\n",
        "image = Image.open(image_path)\n",
        "# Display the Image\n",
        "image"
      ],
      "metadata": {
        "id": "A7ltZkHQcuUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier=pipeline(\"image-classification\")\n",
        "classifier(\"/content/Cat.png\")"
      ],
      "metadata": {
        "id": "tFVILH77cvmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stable Diffusion Pipeline"
      ],
      "metadata": {
        "id": "zcfrpMMVc3GW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install diffusers transformers torch"
      ],
      "metadata": {
        "id": "DbAb3qyic5Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "# Model ID for Stable Diffusion\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "# Load the Stable Diffusion pipeline\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")  # Use GPU for faster inference\n",
        "\n",
        "# Generate an image from text prompt\n",
        "prompt = \"A futuristic cityscape with flying cars\"\n",
        "image = pipe(prompt).images[0]\n",
        "\n",
        "# Display the image (if running in a Jupyter notebook)\n",
        "image.save(\"A.png\")"
      ],
      "metadata": {
        "id": "rgbPHpPVc_VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation Pipeline"
      ],
      "metadata": {
        "id": "k6qb136ddDac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "LrQ-6Qz0dFRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_to_fr = pipeline(\"translation_en_to_fr\")\n",
        "responce=en_to_fr(\"How are you?\")"
      ],
      "metadata": {
        "id": "ufnNs64ldI97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(responce)"
      ],
      "metadata": {
        "id": "n0hdUxQbdNyT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}